import cv2
import numpy as np
from collections import deque
from mmaction.apis import init_recognizer, inference_recognizer
from mmengine.config import Config

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
config_file = '/home/ppds/PycharmProjects/gluh/tsm_mobilenetv2_bukva.py'
checkpoint_file = '/home/ppds/PycharmProjects/mmaction2/work_dirs/tsm_bukva/epoch_50.pth'
device = 'cpu'
clip_len = 8
frame_interval = 2
buffer_size = clip_len * frame_interval

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
config = Config.fromfile(config_file)
model = init_recognizer(config, checkpoint_file, device=device)

# === –ê–ª—Ñ–∞–≤–∏—Ç (–∏–∑ train.txt)
alphabet = list("–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø ")


# === –ö–∞–¥—Ä–æ–≤—ã–π –±—É—Ñ–µ—Ä ===
frame_buffer = deque(maxlen=buffer_size)

# === –í–µ–±-–∫–∞–º–µ—Ä–∞ ===
cap = cv2.VideoCapture(0)
print("üü¢ –ó–∞–ø—É—â–µ–Ω–æ. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")

# === –ö–∞–¥—Ä–æ–≤—ã–π –±—É—Ñ–µ—Ä ===
frame_buffer = deque(maxlen=buffer_size)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
pred_score = 0.0
pred_label = 0


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("üî¥ –ù–µ —É–¥–∞–ª–æ—Å—å —Å—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã.")
        break

    resized = cv2.resize(frame, (340, 256))
    frame_buffer.append(resized)

    if len(frame_buffer) == buffer_size:
        sampled_frames = [frame_buffer[i] for i in range(0, buffer_size, frame_interval)]

        input_data = dict(
            imgs=sampled_frames,
            total_frames=len(sampled_frames),
            label=-1,
            modality='RGB'
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        result = inference_recognizer(model, input_data)
        pred_scores = result.pred_score.cpu().numpy().flatten()
        pred_label = int(np.argmax(pred_scores))
        pred_score = float(np.max(pred_scores))

        print(f'üìç –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {alphabet[pred_label]} (score: {pred_score:.2f})')
        print("üîé –¢–æ–ø-5:")
        top5_indices = np.argsort(pred_scores)[-5:][::-1]
        for i in top5_indices:
            print(f'   {alphabet[i]}: {pred_scores[i]:.2f}')

    # === –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –∫–∞–¥—Ä–µ ===
    display_text = f'{alphabet[pred_label]} ({pred_score:.2f})'
    (text_width, text_height), _ = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 4)
    x, y = 20, 60

    # –§–æ–Ω
    cv2.rectangle(frame, (x - 10, y - text_height - 10), (x + text_width + 10, y + 10), (0, 0, 0), -1)
    # –¢–µ–∫—Å—Ç
    cv2.putText(frame, display_text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–∫–Ω–∞
    cv2.imshow('Live Gesture Recognition', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é
print(f'pred_scores shape: {pred_scores.shape}')
print(f'alphabet length: {len(alphabet)}')

cap.release()
cv2.destroyAllWindows()
